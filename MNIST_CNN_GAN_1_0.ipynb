{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN_GAN_1.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YndnOje6rtKF"
      },
      "source": [
        "%matplotlib inline\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision import datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import pickle as pkl\r\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5DC9R0sEUI"
      },
      "source": [
        "#Hyperparameters\r\n",
        "batch_size = 64\r\n",
        "num_epochs = 100\r\n",
        "learning_rate = 0.0002\r\n",
        "dropout_rate = 0.3\r\n",
        "\r\n",
        "disc_input_size = 784 # size of input to discriminator\r\n",
        "disc_output_size  = 1 # size of discriminator output\r\n",
        "disc_hidden_size = 32 # size of hidden layer for discriminator\r\n",
        "\r\n",
        "latent_size = 100 # size of latent vector to generator \r\n",
        "gen_output_size = 784 # size of generator output\r\n",
        "gen_hidden_size = 32 # size of hidden layer for generator\r\n",
        "\r\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNc_gS04vEXg"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "\r\n",
        "   def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.fc = nn.Linear(100, 256*7*7)\r\n",
        "        self.trans_conv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\r\n",
        "        #self.trans_conv1_bn = nn.BatchNorm2d(128)\r\n",
        "        self.trans_conv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "        #self.trans_conv2_bn = nn.BatchNorm2d(64)\r\n",
        "        self.trans_conv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "        #self.trans_conv3_bn = nn.BatchNorm2d(32)\r\n",
        "        self.trans_conv4 = nn.ConvTranspose2d(32, 1, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\r\n",
        "    \r\n",
        "   def forward(self, x):\r\n",
        "        x = self.fc(x)\r\n",
        "        x = x.view(-1, 256, 7, 7)\r\n",
        "        x = F.relu(self.trans_conv1(x))\r\n",
        "        #x = self.trans_conv1_bn(x)\r\n",
        "        x = F.relu(self.trans_conv2(x))\r\n",
        "        #x = self.trans_conv2_bn(x)\r\n",
        "        x = F.relu(self.trans_conv3(x))\r\n",
        "        #x = self.trans_conv3_bn(x)\r\n",
        "        x = self.trans_conv4(x)\r\n",
        "        x = torch.tanh(x)\r\n",
        "        \r\n",
        "        return x        "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQVmaGhXvzdH"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.conv0 = nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1)\r\n",
        "        #self.conv0_bn = nn.BatchNorm2d(32)\r\n",
        "        self.conv0_drop = nn.Dropout2d(0.25)\r\n",
        "        self.conv1 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "        #self.conv1_bn = nn.BatchNorm2d(64)\r\n",
        "        self.conv1_drop = nn.Dropout2d(0.25)\r\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "        #self.conv2_bn = nn.BatchNorm2d(128)\r\n",
        "        self.conv2_drop = nn.Dropout2d(0.25)\r\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\r\n",
        "        #self.conv3_bn = nn.BatchNorm2d(256)\r\n",
        "        self.conv3_drop = nn.Dropout2d(0.25)\r\n",
        "        self.fc = nn.Linear(12544, 1)\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        x = x.view(-1, 1, 28, 28)\r\n",
        "        x = F.leaky_relu(self.conv0(x), 0.2)\r\n",
        "        #x = self.conv0_bn(x)\r\n",
        "        x = self.conv0_drop(x)\r\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\r\n",
        "        #x = self.conv1_bn(x)\r\n",
        "        x = self.conv1_drop(x)\r\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\r\n",
        "        #x = self.conv2_bn(x)\r\n",
        "        x = self.conv2_drop(x)\r\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\r\n",
        "        #x = self.conv3_bn(x)\r\n",
        "        x = self.conv3_drop(x)\r\n",
        "        x = x.view(-1, self.num_flat_features(x))\r\n",
        "        x = self.fc(x)\r\n",
        "        \r\n",
        "        return x\r\n",
        "    \r\n",
        "    def num_flat_features(self, x):\r\n",
        "        size = x.size()[1:]\r\n",
        "        num_features = 1\r\n",
        "        for s in size:\r\n",
        "            num_features *= s\r\n",
        "        \r\n",
        "        return num_features"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7cac5twwTIW",
        "outputId": "4a1bee39-b4df-4e0d-95c5-956abae3d50d"
      },
      "source": [
        "# instantiating generator and discriminator\r\n",
        "G = Generator()\r\n",
        "D = Discriminator()\r\n",
        "\r\n",
        "\r\n",
        "# printing values of generator and discriminator\r\n",
        "print(D)\r\n",
        "print()\r\n",
        "print(G)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv0_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv1_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv3_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=12544, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (fc): Linear(in_features=100, out_features=12544, bias=True)\n",
            "  (trans_conv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (trans_conv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (trans_conv3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (trans_conv4): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--_28aUCwoL2"
      },
      "source": [
        "def get_mnist_data():\r\n",
        "  # convertint data to torch.FloatTensor\r\n",
        "  transform = transforms.ToTensor()\r\n",
        "\r\n",
        "  # get the training datasets\r\n",
        "  return datasets.MNIST(root='data', train=True, download=True, transform=transform)  \r\n",
        "  "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-sinnknx4g5"
      },
      "source": [
        "# Calculate losses\r\n",
        "def real_loss(D_out, smooth=False):\r\n",
        "    batch_size = D_out.size(0)\r\n",
        "    if smooth:\r\n",
        "        # smooth, real labels = 0.9\r\n",
        "        labels = torch.ones(batch_size)*0.9\r\n",
        "    else:\r\n",
        "        labels = torch.ones(batch_size) # real labels = 1    \r\n",
        "    # numerically stable loss\r\n",
        "    criterion = nn.BCEWithLogitsLoss()\r\n",
        "    # calculate loss\r\n",
        "    loss = criterion(D_out.squeeze(), labels)\r\n",
        "    return loss\r\n",
        "\r\n",
        "def fake_loss(D_out):\r\n",
        "    batch_size = D_out.size(0)\r\n",
        "    labels = torch.zeros(batch_size) # fake labels = 0\r\n",
        "    criterion = nn.BCEWithLogitsLoss()\r\n",
        "    # calculate loss\r\n",
        "    loss = criterion(D_out.squeeze(), labels)\r\n",
        "    return loss"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arL6WDXWybQi"
      },
      "source": [
        "# Create optimizers for the generator and discriminator \r\n",
        "gen_optimizer = optim.Adam(G.parameters(), learning_rate)\r\n",
        "disc_optimizer = optim.Adam(D.parameters(), learning_rate)\r\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHMBSRt1znYV"
      },
      "source": [
        "# keep track of loss and generated, \"fake\" samples\r\n",
        "samples = []\r\n",
        "losses = []\r\n",
        "\r\n",
        "print_every = 1000\r\n",
        "\r\n",
        "# Get some fixed data for sampling. These are images that are held\r\n",
        "# constant throughout training, and allow us to inspect the model's performance\r\n",
        "sample_size=16\r\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, latent_size))\r\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\r\n",
        "\r\n",
        "#prepare the data loader\r\n",
        "train_loader = torch.utils.data.DataLoader(get_mnist_data(), batch_size=batch_size)\r\n",
        "# train the network\r\n",
        "D.train()\r\n",
        "G.train()\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    \r\n",
        "    for batch_i, (real_images, _) in enumerate(train_loader):\r\n",
        "                \r\n",
        "        batch_size = real_images.size(0)\r\n",
        "        \r\n",
        "        ## Important rescaling step ## \r\n",
        "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\r\n",
        "        \r\n",
        "        #TRAINING DISCRIMINATOR\r\n",
        "        \r\n",
        "        disc_optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # 1. Train with real images\r\n",
        "\r\n",
        "        # Compute the discriminator losses on real images \r\n",
        "        # smooth the real labels\r\n",
        "        D_real = D(real_images)\r\n",
        "        d_real_loss = real_loss(D_real, smooth=True)\r\n",
        "        \r\n",
        "        # 2. Train with fake images\r\n",
        "        \r\n",
        "        # Generate fake images\r\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, latent_size))\r\n",
        "        z = torch.from_numpy(z).float()\r\n",
        "        fake_images = G(z)\r\n",
        "        \r\n",
        "        # Compute the discriminator losses on fake images        \r\n",
        "        D_fake = D(fake_images)\r\n",
        "        d_fake_loss = fake_loss(D_fake)\r\n",
        "        \r\n",
        "        # add up loss and perform backprop\r\n",
        "        d_loss = d_real_loss + d_fake_loss\r\n",
        "        d_loss.backward()\r\n",
        "        disc_optimizer.step()\r\n",
        "        \r\n",
        "        \r\n",
        "        # TRAINING GENERATOR\r\n",
        "        gen_optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # 1. Train with fake images and flipped labels\r\n",
        "        \r\n",
        "        # Generate fake images\r\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, latent_size))\r\n",
        "        z = torch.from_numpy(z).float()\r\n",
        "        fake_images = G(z)\r\n",
        "        \r\n",
        "        # Compute the discriminator losses on fake images \r\n",
        "        # using flipped labels!\r\n",
        "        D_fake = D(fake_images)\r\n",
        "        g_loss = real_loss(D_fake) # use real loss to flip labels\r\n",
        "        \r\n",
        "        # perform backprop\r\n",
        "        g_loss.backward()\r\n",
        "        gen_optimizer.step()\r\n",
        "\r\n",
        "        # Print some loss stats\r\n",
        "        if batch_i % print_every == 0:\r\n",
        "            # print discriminator and generator loss\r\n",
        "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\r\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\r\n",
        "\r\n",
        "    \r\n",
        "    ## AFTER EACH EPOCH##\r\n",
        "    # append discriminator loss and generator loss\r\n",
        "    losses.append((d_loss.item(), g_loss.item()))\r\n",
        "    \r\n",
        "    # generate and save sample, fake images\r\n",
        "    G.eval() # eval mode for generating samples\r\n",
        "    samples_z = G(fixed_z)\r\n",
        "    samples.append(samples_z)\r\n",
        "    G.train() # back to train mode\r\n",
        "\r\n",
        "\r\n",
        "# Save training generator samples\r\n",
        "with open('train_samples.pkl', 'wb') as f:\r\n",
        "    pkl.dump(samples, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYMEeBv71eDU"
      },
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "losses = np.array(losses)\r\n",
        "plt.plot(losses.T[0], label='Discriminator')\r\n",
        "plt.plot(losses.T[1], label='Generator')\r\n",
        "plt.title(\"Training Losses\")\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHpYRYiPQFrL"
      },
      "source": [
        "# helper function for viewing a list of passed in sample images\r\n",
        "def view_samples(epoch, samples):\r\n",
        "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\r\n",
        "    for ax, img in zip(axes.flatten(), samples[epoch]):\r\n",
        "        img = img.detach()\r\n",
        "        ax.xaxis.set_visible(False)\r\n",
        "        ax.yaxis.set_visible(False)\r\n",
        "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KEu3Qz5ZTHz"
      },
      "source": [
        "# Load samples from generator, taken while training\r\n",
        "with open('train_samples.pkl', 'rb') as f:\r\n",
        "    samples = pkl.load(f)\r\n",
        "# -1 indicates final epoch's samples (the last in the list)\r\n",
        "view_samples(-1, samples)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjLhxXWVZWqI"
      },
      "source": [
        "rows = 10 # split epochs into 10, so 100/10 = every 10 epochs\r\n",
        "cols = 6\r\n",
        "fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\r\n",
        "\r\n",
        "for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\r\n",
        "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\r\n",
        "        img = img.detach()\r\n",
        "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\r\n",
        "        ax.xaxis.set_visible(False)\r\n",
        "        ax.yaxis.set_visible(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__di912sZbGF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}